# ğŸ§  Ollama CLI Chat

![Platforms](https://img.shields.io/badge/platforms-linux%20%7C%20macOS%20%7C%20windows-blue?style=flat-square)
>  ![Go Version](https://img.shields.io/badge/Go-1.22+-00ADD8?style=flat-square&logo=go)
> ![Status](https://img.shields.io/badge/status-active-brightgreen?style=flat-square)
> ![Build](https://img.shields.io/badge/build-passing-success?style=flat-square)




> A clean, minimal **CLI application** to interact with local Ollama models â€” featuring session management, chat persistence, and resume functionality.


## âœ¨ Features

- ğŸ” **Start New Chats** with any supported local Ollama model.
- ğŸ’¾ **Save Chat Sessions** automatically with timestamp and model.
- â™»ï¸ **Resume Previous Chats** seamlessly â€” continue from where you left off.
- ğŸ§¹ **Delete Chats** with interactive selection.
- ğŸ“œ **View Past Chat Messages** before resuming sessions.
- âœ… Fully persistent and local â€“ **no cloud or external API** dependency.
- ğŸ§ª Built with `cobra` for a user-friendly CLI experience.
- ğŸ§  Supports **streamed response generation** via `Ollama` API.



## ğŸ“¦ Installation
> **ğŸ”¨ Option 1: Download Prebuilt Binaries (Recommended)**

Available for: Windows, macOS (Intel & ARM), Linux

Go to the Releases directory and download the binary for your OS:

> OS Files
- Windows	     `ollama-cli_windows_amd64.exe`
- macOS	`ollama-cli_darwin_amd64 or ollama-cli_darwin_arm64`
- Linux	`ollama-cli_linux_amd64`

After downloading:

Before running the cli make sure the ollama is running.
```
# (macOS/Linux)
chmod +x ollama-cli
./ollama-cli
```

```
# (Windows)
Open cmd and navigate to Directory where the .exe is.
.\ollama-cli.exe
```
> **âš™ï¸ Option 2: Build from Source** 

> **Requirements**: Go 1.22+, [Ollama](https://ollama.com) installed and running locally.

âš ï¸ **Important**: Ollama **must be running locally on the default URL** (`http://localhost:11434`) for this CLI to function correctly.

Start Ollama in a separate terminal before using the CLI:

```bash
ollama serve
```
Then build and run the CLI:
```bash
git clone https://github.com/Ashank007/ollama-cli.git
cd ollama-cli
go build -o ollama-cli
./ollama-cli

```
Or run directly:
```bash
go run .
```
## ğŸš€ Usage
Start the CLI:
```bash
./ollama-cli chat
```
You'll see an interactive menu:
```bash
ğŸ§  What would you like to do?
[1] Start a new chat
[2] Continue a previous chat
[3] Delete a chat
[4] Exit
```
List saved chats:
```bash
./ollama-cli list
```
## ğŸ—ƒï¸ Chat Storage
- Chats are saved locally in the .chats directory.
- Filenames include session name and timestamp.
- All history is preserved in JSON format.

## ğŸ’¬ Commands Overview
Command	Description
``` bash
chat	Start, resume, or delete chat sessions
list	View all saved chat sessions
```
## ğŸ”§ Configuration

You can choose the model interactively when starting a chat, or predefine it using:

```
./ollama-cli chat --model llama3
```
## ğŸ“ Project Structure
```
cmd/
â”œâ”€â”€ chat.go               # Chat command & interaction
â”œâ”€â”€ chat_handler.go       # Chat loop logic
â”œâ”€â”€ storage.go            # Chat persistence (save/load/delete)
â”œâ”€â”€ list_chats.go         # List command
â”œâ”€â”€ root.go               # CLI root setup
â”œâ”€â”€ model_picker.go       # Model selection logic
â”œâ”€â”€ types.go       # Message and Session struct definitions
```             

## â“ FAQ

#### Q: Can I use this without internet?
```
Yes, all chats run locally if your model is downloaded.
```
#### Q: Does this support OpenAI or only Ollama?
```
It is made for Ollama Only.
```

## ğŸ§  Under the Hood (Tech Stack)
```
âš™ï¸ Go 1.22
ğŸ spf13/cobra for CLI structure
ğŸ§  Ollama API for model interaction
ğŸ—‚ï¸ JSON-based local chat persistence
```



## ğŸ“œ License
Licensed under the MIT License.

## ğŸ™Œ Acknowledgments
- Powered by Ollama
- CLI framework: spf13/cobra